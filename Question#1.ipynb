{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression as LGR\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_data includes 2400 samples, where each sample is a list including the\n",
    "elements which are the words in reviews.\n",
    "\n",
    "train_label includes 2400 samples which belongs to {0,1}, which is the label \n",
    "of train_data.\n",
    "\n",
    "test_data has the same form as the train_data, while it has 600 sample.\n",
    "\n",
    "test_label is the same as train_label.\n",
    "\"\"\"\n",
    "def Split(filenames):\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    root = \"sentiment labelled sentences/\"\n",
    "    for filename in filenames:\n",
    "        path = root + filename\n",
    "        count = [1,1]\n",
    "        punctuation = [\"!\",\"%\",\"&\",\"(\",\")\",\"+\",\".\",\":\",\";\",\"<\",\"=\",\">\",\"?\",\"*\",\",\",\"\\t\",\"\"]\n",
    "        for line in open(path):\n",
    "            if line[-1] == \"\\n\":\n",
    "                line = line[:-1]\n",
    "            a = int(line[-1])\n",
    "            b=[]\n",
    "            for word in line[:-1].split(' '):\n",
    "                ##while word and word[-1] in punctuation:\n",
    "                    ##word = word[:-1]\n",
    "                ##b.append(wordnet_lemmatizer.lemmatize(word.lower()))\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    if word[i] in punctuation:\n",
    "                        word = word[:i]+word[i+1:]\n",
    "                    else:\n",
    "                        i+=1\n",
    "                c = word.lower()\n",
    "                if c == \"and\" or c == \"or\" or c==\"\":\n",
    "                    continue\n",
    "                b.append(c)\n",
    "            if count[a] > 400:\n",
    "                test_label.append(a)\n",
    "                test_data.append(b)\n",
    "            else:\n",
    "                train_label.append(a)\n",
    "                train_data.append(b)\n",
    "            count[a]+=1\n",
    "    return [train_data, train_label, test_data, test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[train_data, train_label, test_data, test_label] = Split([\"yelp_labelled.txt\",\"amazon_cells_labelled.txt\",\"imdb_labelled.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dic is a dictionary where key is the word shows in train_data and the items\n",
    "of is a list with two elements, first one is the frequency of the key and \n",
    "second element is the index of the key in feature vector, which we will use\n",
    "after.\n",
    "\"\"\"\n",
    "def bagOfWord(data):\n",
    "    dic = {}\n",
    "    t = 0\n",
    "    n = 0\n",
    "    for dataset in data:\n",
    "        for line in dataset:\n",
    "            for word in line:\n",
    "                if word in dic:\n",
    "                    dic[word][0] += 1\n",
    "                elif t == 0:\n",
    "                    dic[word] = [1,n]\n",
    "                    n+=1\n",
    "        t = 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4819"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dic = bagOfWord([train_data, test_data])\n",
    "len(Dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Build feature vector.\"\"\"\n",
    "def buildB(data, dic):\n",
    "    data_b = []\n",
    "    size_dic = len(dic)\n",
    "    for line in data:\n",
    "        temp = [0]*size_dic\n",
    "        for word in line:\n",
    "            if word in dic:\n",
    "                temp[dic[word][1]]+=1.0\n",
    "        data_b.append(np.array(temp))\n",
    "    return data_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 s, sys: 28.3 ms, total: 1.59 s\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%time [train_data_b, test_data_b] = [buildB(train_data,Dic), buildB(test_data,Dic)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "l^2 normalization\n",
    "\"\"\"\n",
    "def l2normalize(data):\n",
    "    for vector in data:\n",
    "        L = np.linalg.norm(vector)\n",
    "        vector /= L\n",
    "                \n",
    "def standardize(data_b, size_dic):\n",
    "    s = np.array([0.0]*size_dic)\n",
    "    for bite in data_b:\n",
    "        s += bite\n",
    "    s_ = s/len(data_b)\n",
    "    vec = []\n",
    "    for bit in data_b:\n",
    "            vec.append(bit - s_)\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_vec and test_vec will be the feature vector to be used for future.\n",
    "\"\"\"\n",
    "l2normalize(train_data_b), l2normalize(test_data_b)\n",
    "[train_vec, test_vec] = [standardize(train_data_b,len(Dic)), standardize(test_data_b,len(Dic))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "randomly pick two points in sample set to be initial points\n",
    "label is a list indicate which cluster the vector is signed to.\n",
    "p is the list including two mean point that the model converget to.\n",
    "During the function, it first prints which two points function pick as\n",
    "initial points and then how many time it iterates.\n",
    "\"\"\"\n",
    "def KMeans_2(data,size_dic):\n",
    "    ##p = kmeans.cluster_centers_\n",
    "    ##label = kmeans.labels_\n",
    "    a = random.randint(0,len(data)-1)\n",
    "    b = random.randint(0,len(data)-1)\n",
    "    while a==b:\n",
    "        b = random.randint(0,len(data)-1)\n",
    "    p = np.array([data[a], data[b]])\n",
    "    print(\"point_init1 is \",a)\n",
    "    print(\"point_init2 is \",b)\n",
    "    label = [0]*len(data)\n",
    "    conver = False\n",
    "    count = 0\n",
    "    while not conver:\n",
    "        count += 1\n",
    "        conver = True\n",
    "        for i in range(len(data)):\n",
    "            d = [0]*2\n",
    "            d[0] = np.linalg.norm(p[0]-data[i])\n",
    "            d[1] = np.linalg.norm(p[1]-data[i])\n",
    "            if d[label[i]] > d[1-label[i]]:\n",
    "                conver = False\n",
    "                label[i] = 1-label[i]\n",
    "        if not conver:\n",
    "            ##print(\"a\")\n",
    "            for j in [0,1]:\n",
    "                n_p = 0\n",
    "                s_p = np.array([0.0]*size_dic)\n",
    "                for point in range(len(label)):\n",
    "                    if label[point] == j:\n",
    "                        s_p += data[point]\n",
    "                        n_p += 1\n",
    "                p[j] = s_p/n_p\n",
    "    print(\"iterate time is \",count)\n",
    "    return(label, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_kmeans(vec, k_train_label,kmeans_lib,size):\n",
    "    [k_label, k_p]=KMeans_2(vec, size)\n",
    "    n_bruce = 0\n",
    "    n_python = 0\n",
    "    for i in range(len(k_label)):\n",
    "        if k_train_label[i] == kmeans_lib.labels_[i]:\n",
    "            n_python+=1\n",
    "        if k_train_label[i] == k_label[i]:\n",
    "            n_bruce+=1\n",
    "    print(\"self-designed accuracy is\",n_bruce/len(k_label))\n",
    "    print(\"          lib accuracy is\", n_python/len(k_label))\n",
    "    print(\"higher than lib?: \",n_python/len(k_label)<n_bruce/len(k_label) )\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  587\n",
      "point_init2 is  1305\n",
      "iterate time is  10\n",
      "self-designed accuracy is 0.5054166666666666\n",
      "          lib accuracy is 0.5058333333333334\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  694\n",
      "point_init2 is  947\n",
      "iterate time is  12\n",
      "self-designed accuracy is 0.49541666666666667\n",
      "          lib accuracy is 0.5058333333333334\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  1302\n",
      "point_init2 is  844\n",
      "iterate time is  13\n",
      "self-designed accuracy is 0.51375\n",
      "          lib accuracy is 0.5058333333333334\n",
      "higher than lib?:  True\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    n_kmeans(train_vec, train_label, kmeans,len(Dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79166666666666663"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = LGR()\n",
    "lgr.fit(train_vec ,train_label)\n",
    "lgr.score(test_vec,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Ngram(data):\n",
    "    data_ng = []\n",
    "    for line in data:\n",
    "        line_new = []\n",
    "        for i in range(len(line)-1):\n",
    "            line_new.append(line[i]+' '+line[i+1])\n",
    "        data_ng.append(line_new)\n",
    "    return data_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_ng = Ngram(train_data)\n",
    "test_data_ng = Ngram(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16621"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dic_ng = bagOfWord([train_data_ng, test_data_ng])\n",
    "len(Dic_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.38 s, sys: 911 ms, total: 6.29 s\n",
      "Wall time: 6.35 s\n"
     ]
    }
   ],
   "source": [
    "%time [train_data_ng_b, test_data_ng_b] = [buildB(train_data_ng,Dic_ng), buildB(test_data_ng,Dic_ng)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##drop empty element\n",
    "train_label_ng = train_label[:]\n",
    "i = 0\n",
    "while i < len(train_data_ng_b):\n",
    "    if not np.linalg.norm(train_data_ng_b[i]):\n",
    "        train_label_ng.pop(i)\n",
    "        train_data_ng_b.pop(i)\n",
    "        train_data_ng.pop(i)\n",
    "    else:\n",
    "        i+=1\n",
    "        \n",
    "test_label_ng = test_label[:]\n",
    "i = 0\n",
    "while i < len(test_data_ng_b):\n",
    "    if not np.linalg.norm(test_data_ng_b[i]):\n",
    "        test_label_ng.pop(i)\n",
    "        test_data_ng_b.pop(i)\n",
    "        test_data_ng.pop(i)\n",
    "    else:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l2normalize(train_data_ng_b)\n",
    "l2normalize(test_data_ng_b)\n",
    "[train_vec_ng, test_vec_ng] = [standardize(train_data_ng_b, len(Dic_ng)), standardize(test_data_ng_b, len(Dic_ng))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_ng = KMeans(n_clusters=2, random_state=0).fit(train_vec_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  812\n",
      "point_init2 is  2358\n",
      "iterate time is  7\n",
      "self-designed accuracy is 0.5121338912133891\n",
      "          lib accuracy is 0.49874476987447697\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  1910\n",
      "point_init2 is  2304\n",
      "iterate time is  7\n",
      "self-designed accuracy is 0.501673640167364\n",
      "          lib accuracy is 0.49874476987447697\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  1830\n",
      "point_init2 is  224\n",
      "iterate time is  9\n",
      "self-designed accuracy is 0.5359832635983264\n",
      "          lib accuracy is 0.49874476987447697\n",
      "higher than lib?:  True\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    n_kmeans(train_vec_ng, train_label_ng, kmeans_ng, len(Dic_ng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7436823104693141"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = LGR()\n",
    "lgr.fit(train_vec_ng ,train_label_ng)\n",
    "lgr.score(test_vec_ng,test_label_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0703402281144394e-12"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(sum(train_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vec = np.array(train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(train_vec, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2400,), (4819, 4819))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22109504,  0.07218909, -0.1547806 ,  0.12780164, -0.1292377 ])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec[0].dot(V[:5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reduce(n):\n",
    "    \n",
    "    train_vec_n = train_vec.dot(V[:n].T)\n",
    "    test_vec_n = test_vec.dot(V[:n].T)\n",
    "    kmeans_n = KMeans(n_clusters=2, random_state=0).fit(train_vec_n)\n",
    "    for i in range(3):\n",
    "        n_kmeans(train_vec_n, train_label, kmeans_n, n)\n",
    "    lgr = LGR()\n",
    "    lgr.fit(train_vec_n ,train_label)\n",
    "    print(\"Logistic Regression result is\",lgr.score(test_vec_n,test_label))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reduce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  2387\n",
      "point_init2 is  1406\n",
      "iterate time is  8\n",
      "self-designed accuracy is 0.4945833333333333\n",
      "          lib accuracy is 0.4945833333333333\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  1207\n",
      "point_init2 is  1720\n",
      "iterate time is  11\n",
      "self-designed accuracy is 0.50625\n",
      "          lib accuracy is 0.4945833333333333\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  1105\n",
      "point_init2 is  656\n",
      "iterate time is  10\n",
      "self-designed accuracy is 0.5154166666666666\n",
      "          lib accuracy is 0.4945833333333333\n",
      "higher than lib?:  True\n",
      "************************\n",
      "Logistic Regression result is 0.531666666667\n"
     ]
    }
   ],
   "source": [
    "reduce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  706\n",
      "point_init2 is  1996\n",
      "iterate time is  10\n",
      "self-designed accuracy is 0.50625\n",
      "          lib accuracy is 0.5045833333333334\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  14\n",
      "point_init2 is  308\n",
      "iterate time is  15\n",
      "self-designed accuracy is 0.53\n",
      "          lib accuracy is 0.5045833333333334\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  499\n",
      "point_init2 is  84\n",
      "iterate time is  12\n",
      "self-designed accuracy is 0.5045833333333334\n",
      "          lib accuracy is 0.5045833333333334\n",
      "higher than lib?:  False\n",
      "************************\n",
      "Logistic Regression result is 0.678333333333\n"
     ]
    }
   ],
   "source": [
    "reduce(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  1220\n",
      "point_init2 is  1866\n",
      "iterate time is  10\n",
      "self-designed accuracy is 0.46958333333333335\n",
      "          lib accuracy is 0.505\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  576\n",
      "point_init2 is  451\n",
      "iterate time is  10\n",
      "self-designed accuracy is 0.48583333333333334\n",
      "          lib accuracy is 0.505\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  2270\n",
      "point_init2 is  1235\n",
      "iterate time is  9\n",
      "self-designed accuracy is 0.48541666666666666\n",
      "          lib accuracy is 0.505\n",
      "higher than lib?:  False\n",
      "************************\n",
      "Logistic Regression result is 0.71\n"
     ]
    }
   ],
   "source": [
    "reduce(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ..., \n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##compare with PCA Lib\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(train_vec)\n",
    "abs(abs(V[:100])-abs(pca.components_)) < 0.0000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
