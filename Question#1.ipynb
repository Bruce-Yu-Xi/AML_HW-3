{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression as LGR\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_data includes 2400 samples, where each sample is a list including the\n",
    "elements which are the words in reviews.\n",
    "\n",
    "train_label includes 2400 samples which belongs to {0,1}, which is the label \n",
    "of train_data.\n",
    "\n",
    "test_data has the same form as the train_data, while it has 600 sample.\n",
    "\n",
    "test_label is the same as train_label.\n",
    "\"\"\"\n",
    "def Split(filenames):\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    root = \"sentiment_labelled_sentences/\"\n",
    "    for filename in filenames:\n",
    "        path = root + filename\n",
    "        count = [1,1]\n",
    "        punctuation = [\"!\",\"%\",\"&\",\"(\",\")\",\"+\",\".\",\":\",\";\",\"<\",\"=\",\">\",\"?\",\"*\",\",\",\"\\t\",\"\"]\n",
    "        for line in open(path):\n",
    "            if line[-1] == \"\\n\":\n",
    "                line = line[:-1]\n",
    "            a = int(line[-1])\n",
    "            b=[]\n",
    "            for word in line[:-1].split(' '):\n",
    "                ##while word and word[-1] in punctuation:\n",
    "                    ##word = word[:-1]\n",
    "                ##b.append(wordnet_lemmatizer.lemmatize(word.lower()))\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    if word[i] in punctuation:\n",
    "                        word = word[:i]+word[i+1:]\n",
    "                    else:\n",
    "                        i+=1\n",
    "                c = word.lower()\n",
    "                if c == \"and\" or c == \"or\" or c==\"\":\n",
    "                    continue\n",
    "                b.append(c)\n",
    "            if count[a] > 400:\n",
    "                test_label.append(a)\n",
    "                test_data.append(b)\n",
    "            else:\n",
    "                train_label.append(a)\n",
    "                train_data.append(b)\n",
    "            count[a]+=1\n",
    "    return [train_data, train_label, test_data, test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[train_data, train_label, test_data, test_label] = Split([\"yelp_labelled.txt\",\"amazon_cells_labelled.txt\",\"imdb_labelled.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dic is a dictionary where key is the word shows in train_data and the items\n",
    "of is a list with two elements, first one is the frequency of the key and \n",
    "second element is the index of the key in feature vector, which we will use\n",
    "after.\n",
    "\"\"\"\n",
    "def bagOfWord(data):\n",
    "    dic = {}\n",
    "    t = 0\n",
    "    n = 0\n",
    "    for dataset in data:\n",
    "        for line in dataset:\n",
    "            for word in line:\n",
    "                if word in dic:\n",
    "                    dic[word][0] += 1\n",
    "                elif t == 0:\n",
    "                    dic[word] = [1,n]\n",
    "                    n+=1\n",
    "        t = 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4819"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dic = bagOfWord([train_data, test_data])\n",
    "len(Dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Build feature vector.\"\"\"\n",
    "def buildB(data, dic):\n",
    "    data_b = []\n",
    "    size_dic = len(dic)\n",
    "    for line in data:\n",
    "        temp = [0]*size_dic\n",
    "        for word in line:\n",
    "            if word in dic:\n",
    "                temp[dic[word][1]]+=1.0\n",
    "        data_b.append(np.array(temp))\n",
    "    return data_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.62 s, sys: 60.3 ms, total: 1.68 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%time [train_data_b, test_data_b] = [buildB(train_data,Dic), buildB(test_data,Dic)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "l^2 normalization\n",
    "\"\"\"\n",
    "def l2normalize(data):\n",
    "    for vector in data:\n",
    "        L = np.linalg.norm(vector)\n",
    "        vector /= L\n",
    "                \n",
    "def standardize(data_b, size_dic):\n",
    "    s = np.array([0.0]*size_dic)\n",
    "    for bite in data_b:\n",
    "        s += bite\n",
    "    s_ = s/len(data_b)\n",
    "    vec = []\n",
    "    for bit in data_b:\n",
    "            vec.append(bit - s_)\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_vec and test_vec will be the feature vector to be used for future.\n",
    "\"\"\"\n",
    "l2normalize(train_data_b), l2normalize(test_data_b)\n",
    "[train_vec, test_vec] = [standardize(train_data_b,len(Dic)), standardize(test_data_b,len(Dic))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "randomly pick two points in sample set to be initial points\n",
    "label is a list indicate which cluster the vector is signed to.\n",
    "p is the list including two mean point that the model converget to.\n",
    "During the function, it first prints which two points function pick as\n",
    "initial points and then how many time it iterates.\n",
    "\"\"\"\n",
    "def KMeans_2(data,size_dic):\n",
    "    ##p = kmeans.cluster_centers_\n",
    "    ##label = kmeans.labels_\n",
    "    a = random.randint(0,len(data)-1)\n",
    "    b = random.randint(0,len(data)-1)\n",
    "    while a==b:\n",
    "        b = random.randint(0,len(data)-1)\n",
    "    p = np.array([data[a], data[b]])\n",
    "    print(\"point_init1 is \",a)\n",
    "    print(\"point_init2 is \",b)\n",
    "    label = [0]*len(data)\n",
    "    conver = False\n",
    "    count = 0\n",
    "    while not conver:\n",
    "        count += 1\n",
    "        conver = True\n",
    "        for i in range(len(data)):\n",
    "            d = [0]*2\n",
    "            d[0] = np.linalg.norm(p[0]-data[i])\n",
    "            d[1] = np.linalg.norm(p[1]-data[i])\n",
    "            if d[label[i]] > d[1-label[i]]:\n",
    "                conver = False\n",
    "                label[i] = 1-label[i]\n",
    "        if not conver:\n",
    "            ##print(\"a\")\n",
    "            for j in [0,1]:\n",
    "                n_p = 0\n",
    "                s_p = np.array([0.0]*size_dic)\n",
    "                for point in range(len(label)):\n",
    "                    if label[point] == j:\n",
    "                        s_p += data[point]\n",
    "                        n_p += 1\n",
    "                p[j] = s_p/n_p\n",
    "    print(\"iterate time is \",count)\n",
    "    return(label, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_kmeans(vec, k_train_label,kmeans_lib,size):\n",
    "    [k_label, k_p]=KMeans_2(vec, size)\n",
    "    n_bruce = 0\n",
    "    n_python = 0\n",
    "    for i in range(len(k_label)):\n",
    "        if k_train_label[i] == kmeans_lib.labels_[i]:\n",
    "            n_python+=1\n",
    "        if k_train_label[i] == k_label[i]:\n",
    "            n_bruce+=1\n",
    "    print(\"self-designed accuracy is\",n_bruce/len(k_label))\n",
    "    print(\"          lib accuracy is\", n_python/len(k_label))\n",
    "    print(\"higher than lib?: \",n_python/len(k_label)<n_bruce/len(k_label) )\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  1473\n",
      "point_init2 is  633\n",
      "iterate time is  9\n",
      "self-designed accuracy is 0.5058333333333334\n",
      "          lib accuracy is 0.5045833333333334\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  139\n",
      "point_init2 is  2058\n",
      "iterate time is  25\n",
      "self-designed accuracy is 0.49416666666666664\n",
      "          lib accuracy is 0.5045833333333334\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  320\n",
      "point_init2 is  2319\n",
      "iterate time is  11\n",
      "self-designed accuracy is 0.4945833333333333\n",
      "          lib accuracy is 0.5045833333333334\n",
      "higher than lib?:  False\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    n_kmeans(train_vec, train_label, kmeans,len(Dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79166666666666663"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = LGR()\n",
    "lgr.fit(train_vec ,train_label)\n",
    "lgr.score(test_vec,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Ngram(data):\n",
    "    data_ng = []\n",
    "    for line in data:\n",
    "        line_new = []\n",
    "        for i in range(len(line)-1):\n",
    "            line_new.append(line[i]+' '+line[i+1])\n",
    "        data_ng.append(line_new)\n",
    "    return data_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_ng = Ngram(train_data)\n",
    "test_data_ng = Ngram(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16621"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dic_ng = bagOfWord([train_data_ng, test_data_ng])\n",
    "len(Dic_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.46 s, sys: 222 ms, total: 5.69 s\n",
      "Wall time: 5.85 s\n"
     ]
    }
   ],
   "source": [
    "%time [train_data_ng_b, test_data_ng_b] = [buildB(train_data_ng,Dic_ng), buildB(test_data_ng,Dic_ng)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##drop empty element\n",
    "train_label_ng = train_label[:]\n",
    "i = 0\n",
    "while i < len(train_data_ng_b):\n",
    "    if not np.linalg.norm(train_data_ng_b[i]):\n",
    "        train_label_ng.pop(i)\n",
    "        train_data_ng_b.pop(i)\n",
    "        train_data_ng.pop(i)\n",
    "    else:\n",
    "        i+=1\n",
    "        \n",
    "test_label_ng = test_label[:]\n",
    "i = 0\n",
    "while i < len(test_data_ng_b):\n",
    "    if not np.linalg.norm(test_data_ng_b[i]):\n",
    "        test_label_ng.pop(i)\n",
    "        test_data_ng_b.pop(i)\n",
    "        test_data_ng.pop(i)\n",
    "    else:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l2normalize(train_data_ng_b)\n",
    "l2normalize(test_data_ng_b)\n",
    "[train_vec_ng, test_vec_ng] = [standardize(train_data_ng_b, len(Dic_ng)), standardize(test_data_ng_b, len(Dic_ng))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_ng = KMeans(n_clusters=2, random_state=0).fit(train_vec_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  1973\n",
      "point_init2 is  1522\n",
      "iterate time is  6\n",
      "self-designed accuracy is 0.5062761506276151\n",
      "          lib accuracy is 0.497489539748954\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  494\n",
      "point_init2 is  648\n",
      "iterate time is  4\n",
      "self-designed accuracy is 0.497071129707113\n",
      "          lib accuracy is 0.497489539748954\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  2227\n",
      "point_init2 is  1207\n",
      "iterate time is  30\n",
      "self-designed accuracy is 0.4866108786610879\n",
      "          lib accuracy is 0.497489539748954\n",
      "higher than lib?:  False\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    n_kmeans(train_vec_ng, train_label_ng, kmeans_ng, len(Dic_ng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7436823104693141"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = LGR()\n",
    "lgr.fit(train_vec_ng ,train_label_ng)\n",
    "lgr.score(test_vec_ng,test_label_ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(train_vec, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2400,), (4819, 4819))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22109504,  0.07218909, -0.1547806 ,  0.12780164, -0.1292377 ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec[0].dot(V[:5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reduce(n):\n",
    "    \n",
    "    train_vec_n = train_vec.dot(V[:n].T)\n",
    "    test_vec_n = test_vec.dot(V[:n].T)\n",
    "    kmeans_n = KMeans(n_clusters=2, random_state=0).fit(train_vec_n)\n",
    "    for i in range(3):\n",
    "        n_kmeans(train_vec_n, train_label, kmeans_n, n)\n",
    "    lgr = LGR()\n",
    "    lgr.fit(train_vec_n ,train_label)\n",
    "    print(\"Logistic Regression result is\",lgr.score(test_vec_n,test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  749\n",
      "point_init2 is  102\n",
      "iterate time is  6\n",
      "self-designed accuracy is 0.5058333333333334\n",
      "          lib accuracy is 0.4945833333333333\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  1877\n",
      "point_init2 is  1570\n",
      "iterate time is  13\n",
      "self-designed accuracy is 0.5054166666666666\n",
      "          lib accuracy is 0.4945833333333333\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  1213\n",
      "point_init2 is  215\n",
      "iterate time is  10\n",
      "self-designed accuracy is 0.5054166666666666\n",
      "          lib accuracy is 0.4945833333333333\n",
      "higher than lib?:  True\n",
      "************************\n",
      "Logistic Regression result is 0.531666666667\n"
     ]
    }
   ],
   "source": [
    "reduce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  221\n",
      "point_init2 is  453\n",
      "iterate time is  7\n",
      "self-designed accuracy is 0.47583333333333333\n",
      "          lib accuracy is 0.49541666666666667\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  695\n",
      "point_init2 is  457\n",
      "iterate time is  7\n",
      "self-designed accuracy is 0.48291666666666666\n",
      "          lib accuracy is 0.49541666666666667\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  282\n",
      "point_init2 is  2346\n",
      "iterate time is  13\n",
      "self-designed accuracy is 0.5054166666666666\n",
      "          lib accuracy is 0.49541666666666667\n",
      "higher than lib?:  True\n",
      "************************\n",
      "Logistic Regression result is 0.678333333333\n"
     ]
    }
   ],
   "source": [
    "reduce(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_init1 is  303\n",
      "point_init2 is  1436\n",
      "iterate time is  8\n",
      "self-designed accuracy is 0.5141666666666667\n",
      "          lib accuracy is 0.505\n",
      "higher than lib?:  True\n",
      "************************\n",
      "point_init1 is  2243\n",
      "point_init2 is  193\n",
      "iterate time is  10\n",
      "self-designed accuracy is 0.49416666666666664\n",
      "          lib accuracy is 0.505\n",
      "higher than lib?:  False\n",
      "************************\n",
      "point_init1 is  1470\n",
      "point_init2 is  2138\n",
      "iterate time is  8\n",
      "self-designed accuracy is 0.5304166666666666\n",
      "          lib accuracy is 0.505\n",
      "higher than lib?:  True\n",
      "************************\n",
      "Logistic Regression result is 0.71\n"
     ]
    }
   ],
   "source": [
    "reduce(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ..., \n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       [ True, False, False, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##compare with PCA Lib\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(train_vec)\n",
    "abs(abs(V[:100])-abs(pca.components_)) < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4819)\n",
      "[[ -1.28706044e-03   2.61558542e-03  -1.74683770e-01 ...,   3.66490346e-05\n",
      "    3.66490346e-05   3.66490346e-05]\n",
      " [  1.84696434e-03  -1.39964035e-02   1.17324667e-01 ...,   4.73175327e-04\n",
      "    4.73175327e-04   4.73175327e-04]\n",
      " [ -9.24531446e-04   9.02387769e-03   4.95695086e-01 ...,  -2.63199704e-04\n",
      "   -2.63199704e-04  -2.63199704e-04]\n",
      " ..., \n",
      " [  1.19270156e-02   4.32956614e-02  -3.57706619e-03 ...,  -9.43953461e-05\n",
      "   -9.43953461e-05  -9.43953461e-05]\n",
      " [  5.01019572e-03   7.49187748e-03  -1.43277526e-02 ...,   1.38611941e-03\n",
      "    1.38611941e-03   1.38611941e-03]\n",
      " [ -2.95670350e-03   2.56395753e-02  -3.30487193e-02 ...,   3.52078724e-04\n",
      "    3.52078724e-04   3.52078724e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(pca.components_.shape)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4819, 4819)\n",
      "[[ -1.28706044e-03   2.61558542e-03  -1.74683770e-01 ...,   3.66490346e-05\n",
      "    3.66490346e-05   3.66490346e-05]\n",
      " [  1.84696434e-03  -1.39964035e-02   1.17324667e-01 ...,   4.73175327e-04\n",
      "    4.73175327e-04   4.73175327e-04]\n",
      " [  9.24531451e-04  -9.02387767e-03  -4.95695086e-01 ...,   2.63199704e-04\n",
      "    2.63199704e-04   2.63199704e-04]\n",
      " ..., \n",
      " [ -1.45283091e-17   2.26856133e-17   7.52704546e-18 ...,   9.02826361e-01\n",
      "   -9.71736387e-02  -9.71736387e-02]\n",
      " [ -1.21430643e-17   2.04421783e-17   8.22804652e-18 ...,  -9.71736387e-02\n",
      "    9.02826361e-01  -9.71736387e-02]\n",
      " [ -1.44469939e-17   2.20349982e-17   8.18424700e-18 ...,  -9.71736387e-02\n",
      "   -9.71736387e-02   9.02826361e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(V.shape)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.44469939e-17   2.20349982e-17   8.18424700e-18 ...,  -9.71736387e-02\n",
      "  -9.71736387e-02   9.02826361e-01]\n"
     ]
    }
   ],
   "source": [
    "print(V[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
