{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # pos -> Part of Speech parameter\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apples', 'NNS'), ('greenish', 'JJ'), ('tallest', 'JJS'), ('run', 'NN'), ('quickly', 'RB'), ('$', '$'), ('$400', 'CD'), ('the', 'DT')]\n",
      "apples n\n",
      "greenish a\n",
      "tallest a\n",
      "run n\n",
      "quickly r\n",
      "$ None\n",
      "$400 None\n",
      "the None\n",
      "['apple', 'greenish', 'tall', 'run', 'quickly', '$', '$400', 'the']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "# mapping picked up from https://github.com/pararthshah/qa-memnn/blob/master/nltk_utils.py\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "'''\n",
    "takes in a list of tokens of length > 0\n",
    "returns a list of the most likely part of speech for the token\n",
    "\n",
    "'''\n",
    "def get_pos(tokens):\n",
    "    pos_list = nltk.pos_tag(tokens)\n",
    "    print(pos_list)\n",
    "    pos_pegs = []\n",
    "    for i, (token, pos) in enumerate(pos_list):        \n",
    "        pos_peg = penn_to_wn(pos)\n",
    "        if pos_peg is not None:\n",
    "            pos_pegs.append(pos_peg)\n",
    "        else:\n",
    "            pos_pegs.append(None)\n",
    "    return list(zip(tokens, pos_pegs))\n",
    "\n",
    "def lemmatize(pos_tagged_tokens):\n",
    "    lemmatized_tokens = []\n",
    "    for i, (token, pos) in enumerate(pos_tagged_tokens):\n",
    "        print(token, pos)\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token, pos=(pos if pos is not None else 'n')))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "#     print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # pos -> Part of Speech parameter\n",
    "pos_tagged_tokens = get_pos(['apples', 'greenish', 'tallest', 'run', 'quickly', '$', '$400', 'the'])\n",
    "lemmatized_tokens = lemmatize(pos_tagged_tokens)\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "# lemmatizer.lemmatize(text, morphy_tag['VB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return wn.NOUN\n",
    "\n",
    "def memoize1(f):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo:\n",
    "            memo[x] = f(x)\n",
    "        return memo[x]\n",
    "    return helper\n",
    "\n",
    "def memoize2(f):\n",
    "    memo = {}\n",
    "    def helper(x,y):\n",
    "        if (x,y) not in memo:\n",
    "            memo[(x,y)] = f(x, y)\n",
    "        return memo[(x,y)]\n",
    "    return helper\n",
    "\n",
    "def stem_word(word):\n",
    "    return nltk.stem.snowball.EnglishStemmer().stem(word)\n",
    "\n",
    "stem_word = memoize1(stem_word)\n",
    "\n",
    "def get_lemma(word, tag):\n",
    "    return WordNetLemmatizer().lemmatize(word, tag)\n",
    "\n",
    "get_lemma = memoize2(get_lemma)\n",
    "\n",
    "def canonicalize_tokens(tokens):\n",
    "    canonical_tokens = []\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    for tag in tags:\n",
    "        wn_tag = penn_to_wn(tag[1])\n",
    "        t = get_lemma(tag[0], wn_tag)\n",
    "        t = stem_word(t)\n",
    "        canonical_tokens.append(t)\n",
    "    return canonical_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appl', 'greenish', 'tall', 'run', 'quick', '$', '$400', 'the']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canonicalize_tokens(['apples', 'greenish', 'tallest', 'run', 'quickly', '$', '$400', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
